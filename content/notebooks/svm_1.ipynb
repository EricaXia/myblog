{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## What is a SVM? \n",
    "\n",
    "A support vector machine is a supervised learning algorithm, that can be used for both classification and regression, but mostly classification. SVMs classify data by finding a hyperplane (\"dividing line\" that splits the input variables) between the classes in the training data. The hyperplane **maximizes the distance between the hyperplane and the closest data points (the \"margin\")**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/469/0*j6b6qNc-E0RfBxFj\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How does the SVM draw the hyperplane?\n",
    "\n",
    "The hyperplane is chosen as the dividing line which separates the data points *as widely as possible*, hence why the margin is maximized. First, the SVM draws \"Support Vectors\", that is, two hyperplanes with one intersecting the first data point of class A and the other intersecting the first data point of class B. Then the final hyperplane is drawn in the middle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How are SVM algorithms implemented in practice?\n",
    "\n",
    "We use something called the *kernel trick* which transformed the lower-dimensional input data set, using linear algebra, into a higher-dimensional space. \n",
    "\n",
    "Why? *So it will be easier to find a hyperplane that can separate the data.* \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/0*ZnINGVLyQZfrcZYG\">\n",
    "\n",
    "How do we implement the kernel trick? The linear SVM can be transformed by computing the **inner product** of any two given observations. The inner product of two input vectors is the sum of each pair of input values multipled together.\n",
    "\n",
    "A [kernel method](https://en.wikipedia.org/wiki/Kernel_method) uses kernel functions to transform the input data. Types of kernel functions used in SVM include:\n",
    "- Linear kernel (as mentioned above, compute the inner product)\n",
    "- Polynomial kernel\n",
    "- RBF (Radial Basis Function) kernel\n",
    "\n",
    "\n",
    "We would need to use polynomial or RBF (more common) if the data set is not linearly separable. \n",
    "\n",
    "### Polynomial kernel\n",
    "Instead of using inner product, we can use a polynomial kernel function to transform the input vectors x_1 and x_2. \n",
    "\n",
    "$$ K(x_1, x_2) = (x_1^Tx_2 + c)^d $$\n",
    "\n",
    "### RBG kernel\n",
    "Defined mathematically as\n",
    "\n",
    "$$ K(x_1, x_2) = exp(-\\frac{|| x_1 - x_2 ||^2}{2\\sigma^2}) $$\n",
    "\n",
    "And note that the $|| x_1 - x_2||^2$ is the squared Euclidean distance between two feature vectors, and $\\sigma$ is a free parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## When would you use SVM over Random Forest?\n",
    "* When the data set is not linearly separable. Then SVM can use the kernel trick, such as with RBF kernel.\n",
    "* When the data is very high-dimensional. For example in text classification and other NLP problems.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=10,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C is the reg. parameter\n",
    "C = 10\n",
    "\n",
    "## for GPC\n",
    "kernel = 1.0 * RBF([1.0, 1.0])\n",
    "\n",
    "models = {\n",
    "    'Linear SVC': SVC(kernel='linear', C=C, probability=True, random_state=0),\n",
    "    'RBF SVC': SVC(kernel='rbf', C=C, probability=True, random_state=0)\n",
    "    # 'GaussianProcessClassifier': GaussianProcessClassifier(kernel) \n",
    "}\n",
    "\n",
    "n_models = len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(X, y, models):\n",
    "    model_probs = []\n",
    "    for index, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Training Accuracy for {name}: {accuracy*100}%\")\n",
    "\n",
    "        \"\"\" Returns probability of the sample for each class in the model. The columns correspond             to the classes in sorted order, as they appear in the attribute classes_. \"\"\"\n",
    "        probs = model.predict_proba(X)  \n",
    "        model_probs.append(probs)\n",
    "\n",
    "        n_classes = np.unique(y_pred).size\n",
    "        print(n_classes)\n",
    "\n",
    "    return model_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['Linear SVC'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy for Linear SVC: 73.2%\n2\nTraining Accuracy for RBF SVC: 98.1%\n2\n"
     ]
    }
   ],
   "source": [
    "model_probs = classify_data(X, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.20415669, 0.79584331],\n",
       "       [0.12557568, 0.87442432],\n",
       "       [0.31298895, 0.68701105],\n",
       "       ...,\n",
       "       [0.26609342, 0.73390658],\n",
       "       [0.0972355 , 0.9027645 ],\n",
       "       [0.468214  , 0.531786  ]])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "linear_svc_probs = model_probs[0]\n",
    "linear_svc_probs\n",
    "### E.g. so the model classifies the first 3 rows as class 1 (the second col) because it has the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Links\n",
    "\n",
    "[sklearn RBF example](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)\n",
    "\n",
    "[sklearn svm classif example](https://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html#sphx-glr-auto-examples-classification-plot-classification-probability-py)\n",
    "\n",
    "https://machinelearningmastery.com/support-vector-machines-for-machine-learning/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Support-vector_machine\n",
    "\n",
    "https://alekhyo.medium.com/interview-questions-on-svm-bf13e5fbcca8\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}